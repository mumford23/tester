{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><u> **Assignment 2: Classification Models** </u></center>\n",
    "## <center> Team: Rivonia Rodeo </center>\n",
    "## <center> Topic: South African SDG Hub </center>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Introduction</u> ##\n",
    "\n",
    "**Copied from the Veridical Data Science article (delete this when done)**\n",
    "\n",
    "We propose the following steps in a notebook:\n",
    "1. Domain problem formulation (narrative).  \n",
    "Clearly state the real-world question and describe prior work related to\n",
    "this question. Indicate how this question can be answered\n",
    "in the context of a model or analysis.\n",
    "\n",
    "2. Data collection and storage (narrative). Describe how\n",
    "the data were generated, including experimental design\n",
    "principles, and reasons why data is relevant to answer the\n",
    "domain question. Describe where data is stored and how\n",
    "it can be accessed by others.\n",
    "\n",
    "3. Data cleaning and preprocessing (narrative, code, visualization).\n",
    "Describe steps taken to convert raw data into\n",
    "data used for analysis, and why these preprocessing steps\n",
    "are justified. Ask whether more than one preprocessing\n",
    "methods should be used and examine their impacts on\n",
    "the final data results.\n",
    "\n",
    "4. Exploratory data analysis (narrative, code, visualization).\n",
    "Describe any preliminary analyses that influenced modeling\n",
    "decisions or conclusions along with code and visualizations\n",
    "to support these decisions.\n",
    "\n",
    "5. Modeling and Post-hoc analysis (narrative, code, visualization).\n",
    "Carry out PCS inference in the context of the\n",
    "domain question. Specify appropriate model and data\n",
    "perturbations. If necessary, specify null hypotheses and\n",
    "associated perturbations.\n",
    "\n",
    "6. Interpretation of results (narrative and visualization).\n",
    "Translate the data results to draw conclusions and/or\n",
    "make recommendations in the context of domain problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Domain problem formulation  \n",
    "**Goal 1: To develop and train a classifier on the articles in the static database based on the 17 SDGs to improve on the performance of the existing model.**\n",
    "\n",
    "Prior to this analysis, an exploratory data analysis was performed to assess the data and what models would be best suited to achieving the goal for this problem. A classification model was identified as being most suited and naturual language processing techniques to pre-process the abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data collection and storage  \n",
    "The data consists of abstracts of articles, the source of the article as well as its classification into one of the 17 sustainable development goals. Some of the articles are further classified into the sustainable development targets, a further set of achievales within each of the SDGs.  \n",
    "\n",
    "The data was collected by a third party from five different internet sources. A classification model is needed to improve how each of the articles are classified into these 17 SDGs. The data is stored by the University of Pretoria but can be easily accessed from online sources or through the South African SDG Hub website, which categorises and stores wach article used in this study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning and preprocessing\n",
    "Various data cleaning steps are necessary to be able to model the abstract which are text data. Natural languaugae processing techniques are used to clean and prepare the data for the classification analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy import stats\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\TP659BF\\Downloads\\Masters\\MIT 808\\Data\\OneHot_Combined_train.tsv', sep = '\\t', encoding='utf-8') \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.dropna() \n",
    "df = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the original data by removing unwanted characters and strings.\n",
    "\n",
    "df_full = data\n",
    "\n",
    "# drop all rows with no text in 'abstract' column\n",
    "df_full.replace(r'\\s+', ' ', regex=True, inplace=True)\n",
    "df_full['abstract'].replace(' ',np.nan,inplace=True)\n",
    "df_full = df_full[df_full['abstract'].notnull()]\n",
    "\n",
    "# remove duplicates\n",
    "df_full = df_full.drop_duplicates(subset='abstract')\n",
    "\n",
    "# reindex dataframe\n",
    "df_full = df_full.reset_index(drop=True)\n",
    "\n",
    "# remove HTML\n",
    "def remove_html(text):\n",
    "        soup = BeautifulSoup(text, 'lxml') #install lxml\n",
    "        html_free = soup.get_text()\n",
    "        return html_free\n",
    "df_full['preproc_text'] = df_full['abstract'].apply(remove_html)\n",
    "\n",
    "# remove URLs\n",
    "def remove_urls (text):\n",
    "    url_free = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return(url_free) \n",
    "df_full['preproc_text'] = df_full['abstract'].apply(remove_urls)\n",
    "\n",
    "# remove emails\n",
    "def remove_email(text):\n",
    "        no_mail = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "        return no_mail\n",
    "df_full['preproc_text'] = df_full['preproc_text'].apply(remove_email)\n",
    "\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "        no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "        return no_punct\n",
    "df_full['preproc_text'] = df_full['preproc_text'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "# remove digits separately (may want to keep for identification of SDGs?)\n",
    "def remove_digits(text):\n",
    "        no_digits = ''.join(w for w in text if not w.isdigit())\n",
    "        return no_digits\n",
    "df_full['preproc_text'] = df_full['preproc_text'].apply(remove_digits)\n",
    "\n",
    "\n",
    "# remove single letter words\n",
    "def remove_singletons(text):\n",
    "        no_single = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "        return no_single\n",
    "df_full['preproc_text'] = df_full['preproc_text'].apply(remove_singletons)\n",
    "\n",
    "\n",
    "# tokenize text and make text lowercase (change regex to include other characters)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_full['token_text'] = df_full['preproc_text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# remove stopwords using english dictionary\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "def remove_stopwords(text):\n",
    "        words = [w for w in text if w not in stop_words]\n",
    "        return words\n",
    "df_full['nostop_text'] = df_full['token_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_full\n",
    "df_clean = df_clean[['abstract', 'preproc_text', 'source','SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', 'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17']]\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory data analysis\n",
    "Most of the EDA for this study was done in a previous assignment where a classification model was determined as being the most applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all the words in the column 'abstract'\n",
    "all_words = [word for item in list(df_clean['preproc_text']) for word in item]\n",
    "#all_words\n",
    "#frequency distribution\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "#top_words = fdist.most_common(10000)\n",
    "#print(list(top_words))\n",
    "top_words,_ = zip(*fdist.most_common(10000))\n",
    "top_words = set(top_words)\n",
    "#top_words\n",
    "\n",
    "#Keep the top 10000 most common words \n",
    "def keep_top_words(text):\n",
    "    return [word for word in text if word in top_words]\n",
    "\n",
    "df_clean['preproc_text'] = df_clean['preproc_text'].apply(keep_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling and Post-hoc analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Test and training set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Split the training set into a further validation and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation of results\n",
    "In the results, we can determine the effectivenes of the classification model by assessing the accuracy, predictability, sensitivity and specificity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Structure of the Notebook*  \n",
    "Veridical Data Science Article   \n",
    "<br>\n",
    "- *EDA*  \n",
    "https://medium.com/python-pandemonium/introduction-to-exploratory-data-analysis-in-python-8b6bcb55c190  \n",
    "https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15  \n",
    "https://r4ds.had.co.nz/exploratory-data-analysis.html  \n",
    "https://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d    \n",
    "<br>\n",
    "- *Imbalanced data*  \n",
    "https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18  \n",
    "<br>\n",
    "- *Modelling*   \n",
    "https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validation  \n",
    "<br>\n",
    "- *Semi-supervised learning*  \n",
    "https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
